{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNmkdWoXvkBE"
   },
   "source": [
    "## Image Classification on CIFAR-10\n",
    "In this problem we will explore different deep learning architectures for image classification on the CIFAR-10 dataset. Make sure that you are familiar with torch `Tensor`s, two-dimensional convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`), ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor reshaping (`view`).\n",
    "\n",
    "We will use Colab because it has free GPU runtimes available; GPUs can accelerate training times for this problem by 10-100x. **You will need to enable the GPU runtime to use it**. To do so, click \"Runtime\" above and then \"Change runtime type\". There under hardware accelerator choose \"GPU\".\n",
    "\n",
    "This notebook provides some starter code for the CIFAR-10 problem on HW4, including a completed training loop to assist with some of the Pytorch setup. You'll need to modify this code to implement the layers required for the assignment, but this provides a working training loop to start from.\n",
    "\n",
    "*Note: GPU runtimes are limited on Colab. Limit your training to short-running jobs (around 20mins or less) and spread training out over time, if possible. Colab WILL limit your usage of GPU time, so plan ahead and be prepared to take breaks during training.* We also suggest performing your early coding/sweeps on a small fraction of the dataset (~10%) to minimize training time and GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bb7WymOmv_cx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple, Union, List, Callable\n",
    "from torch.optim import SGD\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SusLoH91CEz"
   },
   "source": [
    "Let's verify that we are using a gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1684391170228,
     "user": {
      "displayName": "Elliott Zackrone",
      "userId": "04505639823729149858"
     },
     "user_tz": 420
    },
    "id": "vmxNdxwvxNs1",
    "outputId": "c22015d4-a225-4397-9f40-015bc5ee34c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "#DEVICE = torch.device(\"mps\")\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbFk2t2RxYcn"
   },
   "source": [
    "To use the GPU you will need to send both the model and data to a device; this transfers the model from its default location on CPU to the GPU.\n",
    "\n",
    "Note that torch operations on Tensors will fail if they are not located on the same device.\n",
    "\n",
    "```python\n",
    "model = model.to(DEVICE)  # Sending a model to GPU\n",
    "\n",
    "for x, y in tqdm(data_loader):\n",
    "  x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "```\n",
    "When reading tensors you may need to send them back to cpu, you can do so with `x = x.cpu()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xODE5P5D1Wwy"
   },
   "source": [
    "Let's load CIFAR-10 data. This is how we load datasets using PyTorch in the real world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9404,
     "status": "ok",
     "timestamp": 1684391179631,
     "user": {
      "displayName": "Elliott Zackrone",
      "userId": "04505639823729149858"
     },
     "user_tz": 420
    },
    "id": "wOvLuZry1cKc",
    "outputId": "443de075-18ca-44e7-edcf-ff60c76a743b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG78KMOj61HJ"
   },
   "source": [
    "Here, we'll use the torch `DataLoader` to wrap our datasets. `DataLoader`s handle batching, shuffling, and iterating over data; they can also be useful for building more complex input pipelines that perform transfoermations such as data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mHIZY4BoRxvC"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])\n",
    "\n",
    "# Create separate dataloaders for the train, test, and validation set\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju8QRAyv2u7F"
   },
   "source": [
    "## For Reference: Logistic Regression\n",
    "\n",
    "This problem is about deep learning architectures, not pytorch. We are providing an implementation of logistic regression using SGD in torch, which can serve as a template for the rest of your implementation in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELjydYRt5Frf"
   },
   "source": [
    "Before we get started, let's take a look at our data to get an understanding of what we are doing. CIFAR-10 is a dataset containing images split into 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1684391203568,
     "user": {
      "displayName": "Elliott Zackrone",
      "userId": "04505639823729149858"
     },
     "user_tz": 420
    },
    "id": "_UU8aIle5m8Q",
    "outputId": "11a4b439-04f1-42e7-a16e-a50f2561f25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single batch of images has shape: torch.Size([128, 3, 32, 32])\n",
      "A single RGB image has 3 channels, width 32, and height 32.\n",
      "Size of a batch of images flattened with view: torch.Size([128, 3072])\n",
      "Size of a batch of images flattened with flatten: torch.Size([128, 3072])\n",
      "True\n",
      "This image is labeled as class deer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs, labels = next(iter(train_loader))\n",
    "print(f\"A single batch of images has shape: {imgs.size()}\")\n",
    "example_image, example_label = imgs[0], labels[0]\n",
    "c, w, h = example_image.size()\n",
    "print(f\"A single RGB image has {c} channels, width {w}, and height {h}.\")\n",
    "\n",
    "# This is one way to flatten our images\n",
    "batch_flat_view = imgs.view(-1, c * w * h)\n",
    "print(f\"Size of a batch of images flattened with view: {batch_flat_view.size()}\")\n",
    "\n",
    "# This is another equivalent way\n",
    "batch_flat_flatten = imgs.flatten(1)\n",
    "print(f\"Size of a batch of images flattened with flatten: {batch_flat_flatten.size()}\")\n",
    "\n",
    "# The new dimension is just the product of the ones we flattened\n",
    "d = example_image.flatten().size()[0]\n",
    "print(c * w * h == d)\n",
    "\n",
    "# View the image\n",
    "t =  torchvision.transforms.ToPILImage()\n",
    "plt.imshow(t(example_image))\n",
    "\n",
    "# These are what the class labels in CIFAR-10 represent. For more information,\n",
    "# visit https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "           \"horse\", \"ship\", \"truck\"]\n",
    "print(f\"This image is labeled as class {classes[example_label]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdBbLQdC_mA_"
   },
   "source": [
    "In this problem, we will attempt to predict what class an image is labeled as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3HQyGMn_42D"
   },
   "source": [
    "First, let's create our model. For a linear model we could flatten the data before passing it into the model, but that is not be the case for the convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5qLVC9PbACDt"
   },
   "outputs": [],
   "source": [
    "def linear_model() -> nn.Module:\n",
    "    \"\"\"Instantiate a linear model and send it to device.\"\"\"\n",
    "    model =  nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d, 10)\n",
    "         )\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd49udL8AZ_E"
   },
   "source": [
    "Let's define a method to train this model using SGD as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YgcFP1-UBj1Z"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module, optimizer: SGD,\n",
    "    train_loader: DataLoader, val_loader: DataLoader,\n",
    "    epochs: int = 20\n",
    ")-> Tuple[List[float], List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Trains a model for the specified number of epochs using the loaders.\n",
    "\n",
    "    Returns: \n",
    "    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    for e in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        # Main training loop; iterate over train_loader. The loop\n",
    "        # terminates when the train loader finishes iterating, which is one epoch.\n",
    "        for (x_batch, labels) in train_loader:\n",
    "            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            labels_pred = model(x_batch)\n",
    "            batch_loss = loss(labels_pred, labels)\n",
    "            train_loss = train_loss + batch_loss.item()\n",
    "\n",
    "            labels_pred_max = torch.argmax(labels_pred, 1)\n",
    "            batch_acc = torch.sum(labels_pred_max == labels)\n",
    "            train_acc = train_acc + batch_acc.item()\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(train_acc / (batch_size * len(train_loader)))\n",
    "\n",
    "        # Validation loop; use .no_grad() context manager to save memory.\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (v_batch, labels) in val_loader:\n",
    "                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)\n",
    "                labels_pred = model(v_batch)\n",
    "                v_batch_loss = loss(labels_pred, labels)\n",
    "                val_loss = val_loss + v_batch_loss.item()\n",
    "\n",
    "                v_pred_max = torch.argmax(labels_pred, 1)\n",
    "                batch_acc = torch.sum(v_pred_max == labels)\n",
    "                val_acc = val_acc + batch_acc.item()\n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "            val_accuracies.append(val_acc / (batch_size * len(val_loader)))\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZUqV2ZrEf-u"
   },
   "source": [
    "For this problem, we will be using SGD. The two hyperparameters for our linear model trained with SGD are the learning rate and momentum. Only learning rate will be searched for in this example.\n",
    "\n",
    "Note: We ask you to plot the accuracies for the best 3 models for each structure, so you will need to return multiple sets of hyperparameters for the homework, or, if you do random search, run your hyperparameter search multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2p-oDqbyhUvG"
   },
   "outputs": [],
   "source": [
    "def parameter_search(\n",
    "    train_loader: DataLoader, \n",
    "    val_loader: DataLoader, \n",
    "    model_fn:Callable[[], nn.Module]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Parameter search for our linear model using SGD.\n",
    "\n",
    "    Args:\n",
    "    train_loader: the train dataloader.\n",
    "    val_loader: the validation dataloader.\n",
    "    model_fn: a function that, when called, returns a torch.nn.Module.\n",
    "\n",
    "    Returns:\n",
    "    The learning rate with the least validation loss.\n",
    "    NOTE: you may need to modify this function to search over and return\n",
    "     other parameters beyond learning rate.\n",
    "    \"\"\"\n",
    "    num_iter = 10 \n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    best_lr = 0.0\n",
    "\n",
    "    lrs = torch.linspace(10 ** (-6), 10 ** (-1), num_iter)\n",
    "\n",
    "    for lr in lrs:\n",
    "        print(f\"trying learning rate {lr}\")\n",
    "        model = model_fn()\n",
    "        optim = SGD(model.parameters(), lr)\n",
    "        \n",
    "        train_loss, train_acc, val_loss, val_acc = train(\n",
    "            model,\n",
    "            optim,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=20\n",
    "            )\n",
    "\n",
    "        if min(val_loss) < best_loss:\n",
    "            best_loss = min(val_loss)\n",
    "            best_lr = lr\n",
    "        \n",
    "    return best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZULxD9sGHm1D"
   },
   "source": [
    "Now that we have everything, we can train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "b854d671884f4665a52c35a60e10ee17",
      "77514a93127f4aa291b13e8b82098334",
      "e5f8331defef4c26b26a87c61ccc26c1",
      "3a03dfc67a28475e9d672a31196db358",
      "fb372b3fbd04411496f3bb0243dbb6e6",
      "6a3b28b18b3e42028e9dd9321dbed77d",
      "0b9c90017289437d83cd678177b6bfb0",
      "ecf7718ece43405eb3f3ae2ebbe82d66",
      "528930246f304d42892b3a2dae7929d1",
      "6721d90bcc3142c69bbd9334814d881c"
     ]
    },
    "id": "azwXX-AEIGKx",
    "outputId": "55b65e28-7d24-4924-e8a7-bbb5b1fc22ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 9.999999974752427e-07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826540cc397242bd97388edc10d87de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.011112000793218613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c5a13e5c543f29f865ce54a8d1641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.02222300134599209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf81e7e4f70f4616a34c6e8bf7cf19db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.033334001898765564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f3c65988dc436dbfb0d9095da52c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.04444500058889389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8b79e82b444913b3ca0ef56f3b0cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.05555599927902222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b615201854434e00b109e033c0eabc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.06666699796915054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b061aacf59794d3aaccc747dd906b477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.07777799665927887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af7376da93d4c51baaaf40c01af1427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.08888900279998779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54243e2612f14d4eb4bacc35175e9f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying learning rate 0.10000000149011612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7d769fc6c40229ab10c1249cd2dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lr = parameter_search(train_loader, val_loader, linear_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDzs8UtJSqsT"
   },
   "outputs": [],
   "source": [
    "model = linear_model()\n",
    "optimizer = SGD(model.parameters(), best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d8e645bd20de40238f70d1b5332af937"
     ]
    },
    "id": "_tk07LfkIPgS",
    "outputId": "a71200be-d711-43ae-9d4a-47387cf8b21a"
   },
   "outputs": [],
   "source": [
    "# We are only using 20 epochs for this example. You may have to use more.\n",
    "train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
    "    model, optimizer, train_loader, val_loader, 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSi0cEADDJcf"
   },
   "source": [
    "Plot the training and validation accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "-i_KVlY3WthL",
    "outputId": "091bfafc-4961-40a9-94ee-896cca4b97c5"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, train_accuracy, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accuracy, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Logistic Regression Accuracy for CIFAR-10 vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu8AaNRl4FDi"
   },
   "source": [
    "The last thing we have to do is evaluate our model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzxYGlYH4MwC"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module, loader: DataLoader\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Computes test loss and accuracy of model on loader.\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (batch, labels) in loader:\n",
    "            batch, labels = batch.to(DEVICE), labels.to(DEVICE)\n",
    "            y_batch_pred = model(batch)\n",
    "            batch_loss = loss(y_batch_pred, labels)\n",
    "            test_loss = test_loss + batch_loss.item()\n",
    "\n",
    "            pred_max = torch.argmax(y_batch_pred, 1)\n",
    "            batch_acc = torch.sum(pred_max == labels)\n",
    "            test_acc = test_acc + batch_acc.item()\n",
    "        test_loss = test_loss / len(loader)\n",
    "        test_acc = test_acc / (batch_size * len(loader))\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekfbYD_34XFB",
    "outputId": "97ef26db-193f-40ba-e9d4-3d005e1ed61a"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK-KuYt41uFA"
   },
   "source": [
    "The rest is yours to code. You can structure the code any way you would like.\n",
    "\n",
    "We do advise making using code cells and functions (train, search, predict etc.) for each subproblem, since they will make your code easier to debug. \n",
    "\n",
    "Also note that several of the functions above can be reused for the various different models you will implement for this problem; i.e., you won't need to write a new `evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6gRW9EyTWFRO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e962fb8be0b46a69be6f5d4f8056025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.2125010788440704\n",
      "Training accuracy: 0.304, Validation accuracy: 0.315\n",
      "Epoch 2, loss: 2.0623603900047867\n",
      "Training accuracy: 0.325, Validation accuracy: 0.342\n",
      "Epoch 3, loss: 1.9511607858267697\n",
      "Training accuracy: 0.339, Validation accuracy: 0.344\n",
      "Epoch 4, loss: 1.8738037825308063\n",
      "Training accuracy: 0.365, Validation accuracy: 0.368\n",
      "Epoch 5, loss: 1.8163763345642523\n",
      "Training accuracy: 0.372, Validation accuracy: 0.374\n",
      "Epoch 6, loss: 1.772021054882895\n",
      "Training accuracy: 0.381, Validation accuracy: 0.385\n",
      "Epoch 7, loss: 1.7350055362013252\n",
      "Training accuracy: 0.396, Validation accuracy: 0.387\n",
      "Epoch 8, loss: 1.7037042379379272\n",
      "Training accuracy: 0.403, Validation accuracy: 0.401\n",
      "Epoch 9, loss: 1.6762387793172489\n",
      "Training accuracy: 0.417, Validation accuracy: 0.412\n",
      "Epoch 10, loss: 1.6524612751196732\n",
      "Training accuracy: 0.427, Validation accuracy: 0.424\n",
      "Epoch 11, loss: 1.6307290684093128\n",
      "Training accuracy: 0.43, Validation accuracy: 0.421\n",
      "Epoch 12, loss: 1.6118919544599273\n",
      "Training accuracy: 0.44, Validation accuracy: 0.433\n",
      "Epoch 13, loss: 1.593603846024383\n",
      "Training accuracy: 0.443, Validation accuracy: 0.432\n",
      "Epoch 14, loss: 1.577697591009465\n",
      "Training accuracy: 0.454, Validation accuracy: 0.453\n",
      "Epoch 15, loss: 1.56195095655593\n",
      "Training accuracy: 0.458, Validation accuracy: 0.452\n",
      "Epoch 16, loss: 1.547418229620565\n",
      "Training accuracy: 0.464, Validation accuracy: 0.455\n",
      "Epoch 17, loss: 1.533562696115537\n",
      "Training accuracy: 0.466, Validation accuracy: 0.463\n",
      "Epoch 18, loss: 1.5203917426141826\n",
      "Training accuracy: 0.469, Validation accuracy: 0.463\n",
      "Epoch 19, loss: 1.5077794499017976\n",
      "Training accuracy: 0.481, Validation accuracy: 0.47\n",
      "Epoch 20, loss: 1.4952740550718524\n",
      "Training accuracy: 0.479, Validation accuracy: 0.472\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from math import floor\n",
    "\n",
    "#DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"mps\")\n",
    "#DEVICE = torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# k is filter size (k x k)\n",
    "# M is number of filters in convnet, or fully connected neurons in fullyconnectednet\n",
    "# N max pool size (N x N)\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        # here M is the number of hidden layer neurons in the fully connected network\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 32 * 3, M, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M, 10, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, M=100, k=5, N=14):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, M, k),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(N),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(M * floor((33 - k) // N) ** 2, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "def compute_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return round(correct / total, 3)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    model = model.to(DEVICE)\n",
    "    for e in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            batch, labels = data\n",
    "            batch, labels = batch.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {e + 1}, loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Compute and print accuracy\n",
    "        train_acc = compute_accuracy(model, train_loader)\n",
    "        val_acc = compute_accuracy(model, val_loader)\n",
    "        print(f'Training accuracy: {train_acc}, Validation accuracy: {val_acc}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    \n",
    "fcmodel = FullyConnectedNet(1000)\n",
    "convnet = ConvNet(M=400, k=5, N=14)\n",
    "optimizer = optim.SGD(convnet.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(convnet, train_loader, val_loader, criterion, optimizer, num_epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mps = 1:53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "bf814fda1cc440ac317e22279a1ec33d1b20faeecc9ea242b28923e33d4f784d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
